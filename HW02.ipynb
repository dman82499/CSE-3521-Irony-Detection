{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2+UN/Hzu8nD1kIApR67U+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dman82499/CSE-3521-Irony-Detection/blob/main/HW02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSpccGY3hnn3",
        "outputId": "a1923d2b-0dad-4daa-8b51-70898ae0aa24"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Import training data and store as DF\n",
        "positive_train_URL = 'https://raw.githubusercontent.com/jeniyat/CSE-5521-SP21/master/HW/HW2/Data/train/Positive.txt'\n",
        "negative_train_URL = 'https://raw.githubusercontent.com/jeniyat/CSE-5521-SP21/master/HW/HW2/Data/train/Negative.txt'\n",
        "neutral_train_URL = 'https://raw.githubusercontent.com/jeniyat/CSE-5521-SP21/master/HW/HW2/Data/train/Neutral.txt'\n",
        "\n",
        "positive_train_DF = pd.read_table(positive_train_URL, header=None);\n",
        "negative_train_DF = pd.read_table(negative_train_URL, header=None);\n",
        "neutral_train_DF = pd.read_table(neutral_train_URL, header=None);\n",
        "\n",
        "# Import testing data and store as DF\n",
        "positive_test_URL = 'https://raw.githubusercontent.com/jeniyat/CSE-5521-SP21/master/HW/HW2/Data/test/Positive.txt'\n",
        "negative_test_URL = 'https://raw.githubusercontent.com/jeniyat/CSE-5521-SP21/master/HW/HW2/Data/test/Negative.txt'\n",
        "neutral_test_URL = 'https://raw.githubusercontent.com/jeniyat/CSE-5521-SP21/master/HW/HW2/Data/test/Neutral.txt'\n",
        "\n",
        "positive_test_DF = pd.read_table(positive_test_URL, header=None);\n",
        "negative_test_DF = pd.read_table(negative_test_URL, header=None);\n",
        "neutral_test_DF = pd.read_table(neutral_test_URL, header=None);\n",
        "\n",
        "# Converts DF into list of strings containing Tweets\n",
        "positive_train = positive_train_DF[0].tolist()\n",
        "negative_train = negative_train_DF[0].tolist()\n",
        "neutral_train = neutral_train_DF[0].tolist()\n",
        "positive_test = positive_test_DF[0].tolist()\n",
        "negative_test = negative_test_DF[0].tolist()\n",
        "neutral_test = neutral_test_DF[0].tolist()\n",
        "\n",
        "num_of_pos_doc = len(positive_train)\n",
        "num_of_neg_doc = len(negative_train)\n",
        "num_of_neu_doc = len(neutral_train)\n",
        "total_num_doc = num_of_neg_doc + num_of_neu_doc + num_of_pos_doc\n",
        "\n",
        "prob_pos = num_of_pos_doc / total_num_doc\n",
        "prob_neg = num_of_neg_doc / total_num_doc\n",
        "prob_neu = num_of_neu_doc / total_num_doc\n",
        "\n",
        "vocab = Counter();\n",
        "vocab_pos = Counter();\n",
        "vocab_neg = Counter();\n",
        "vocab_neu = Counter();\n",
        "\n",
        "total_pos_word = 0;\n",
        "total_neg_word = 0;\n",
        "total_neu_word = 0;\n",
        "\n",
        "# Split positive training data into bag of words\n",
        "for sent in positive_train:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_pos[word] += 1\n",
        "        total_pos_word += 1\n",
        "\n",
        "# Split negative training data into bag of words\n",
        "for sent in negative_train:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_neg[word] += 1\n",
        "        total_neg_word += 1\n",
        "\n",
        "# Split neutral training data into bag of words\n",
        "for sent in neutral_train:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_neu[word] += 1\n",
        "        total_neu_word += 1\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# From testing, alpha = 2 yielded the most accurate results at 0.87613 rounded\n",
        "alpha = 2\n",
        "\n",
        "# Find probability for positive training data\n",
        "def find_pos_prob(list_of_words):\n",
        "    pos_prob_list_of_words = prob_pos\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_pos:\n",
        "            numerator = vocab_pos[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_pos_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        pos_prob_list_of_words = pos_prob_list_of_words * word_prob\n",
        "\n",
        "    return pos_prob_list_of_words\n",
        "\n",
        "# Find probability for negative training data\n",
        "def find_neg_prob(list_of_words):\n",
        "    neg_prob_list_of_words = prob_neg\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_neg:\n",
        "            numerator = vocab_neg[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_neg_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        neg_prob_list_of_words = neg_prob_list_of_words * word_prob\n",
        "\n",
        "    return neg_prob_list_of_words\n",
        "\n",
        "# Find probability for neutral training data\n",
        "def find_neu_prob(list_of_words):\n",
        "    neu_prob_list_of_words = prob_neu\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_neu:\n",
        "            numerator = vocab_neu[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_neu_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        neu_prob_list_of_words = neu_prob_list_of_words * word_prob\n",
        "\n",
        "    return neu_prob_list_of_words\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Positive testing data\n",
        "for sent in positive_test:\n",
        "    word_list = sent.split()\n",
        "    pos_prob_sent = find_pos_prob(word_list)\n",
        "    neg_prob_sent = find_neg_prob(word_list)\n",
        "    neu_prob_sent = find_neu_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"+ve\"\n",
        "\n",
        "    max_prob = max(pos_prob_sent, neg_prob_sent, neu_prob_sent)\n",
        "\n",
        "    if max_prob == pos_prob_sent:\n",
        "        pred_sentiment = \"+ve\"\n",
        "    if max_prob == neg_prob_sent:\n",
        "        pred_sentiment = \"-ve\"\n",
        "    if max_prob == neu_prob_sent:\n",
        "        pred_sentiment = \"0ve\"\n",
        "\n",
        "    predictions.append((sent, true_sentiment, pred_sentiment))\n",
        "\n",
        "# Negative testing data\n",
        "for sent in negative_test:\n",
        "    word_list = sent.split()\n",
        "    pos_prob_sent = find_pos_prob(word_list)\n",
        "    neg_prob_sent = find_neg_prob(word_list)\n",
        "    neu_prob_sent = find_neu_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"-ve\"\n",
        "\n",
        "    max_prob = max(pos_prob_sent, neg_prob_sent, neu_prob_sent)\n",
        "\n",
        "    if max_prob == pos_prob_sent:\n",
        "        pred_sentiment = \"+ve\"\n",
        "    if max_prob == neg_prob_sent:\n",
        "        pred_sentiment = \"-ve\"\n",
        "    if max_prob == neu_prob_sent:\n",
        "        pred_sentiment = \"0ve\"\n",
        "\n",
        "    predictions.append((sent, true_sentiment, pred_sentiment))\n",
        "\n",
        "\n",
        "# Neutral testing data\n",
        "for sent in neutral_test:\n",
        "    word_list = sent.split()\n",
        "    pos_prob_sent = find_pos_prob(word_list)\n",
        "    neg_prob_sent = find_neg_prob(word_list)\n",
        "    neu_prob_sent = find_neu_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"0ve\"\n",
        "\n",
        "    max_prob = max(pos_prob_sent, neg_prob_sent, neu_prob_sent)\n",
        "\n",
        "    if max_prob == pos_prob_sent:\n",
        "        pred_sentiment = \"+ve\"\n",
        "    if max_prob == neg_prob_sent:\n",
        "        pred_sentiment = \"-ve\"\n",
        "    if max_prob == neu_prob_sent:\n",
        "        pred_sentiment = \"0ve\"\n",
        "\n",
        "    predictions.append((sent, true_sentiment, pred_sentiment))\n",
        "\n",
        "correct_pred = 0\n",
        "total_test_case = 0\n",
        "\n",
        "for pred in predictions:\n",
        "    (sent, true_sentiment, pred_sentiment) = pred\n",
        "    if true_sentiment == pred_sentiment:\n",
        "        correct_pred += 1\n",
        "    total_test_case += 1\n",
        "\n",
        "accuracy = correct_pred / total_test_case\n",
        "print(\"Accuracy of \", accuracy)\n",
        "print(\"Alpha is \", alpha)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of  0.8761290322580645\n",
            "Alpha is  2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}