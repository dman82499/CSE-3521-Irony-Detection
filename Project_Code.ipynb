{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMSb8avkjh37v0vZ1mzZJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dman82499/CSE-3521-Irony-Detection/blob/main/Project_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSpccGY3hnn3",
        "outputId": "afa758fe-b12c-40bd-ec42-cf0e5bbb875b"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Import training data for task A and store as DF\n",
        "training_A_URL = 'https://raw.githubusercontent.com/Cyvhee/SemEval2018-Task3/master/datasets/train/SemEval2018-T3-train-taskA_emoji.txt'\n",
        "\n",
        "training_A_DF = pd.read_table(training_A_URL)\n",
        "\n",
        "# Import testing data A and store as DF\n",
        "test_A_URL = 'https://raw.githubusercontent.com/Cyvhee/SemEval2018-Task3/master/datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt'\n",
        "\n",
        "test_A_DF = pd.read_table(test_A_URL)\n",
        "\n",
        "# Converts DF into list of strings containing Tweets separated by irony classifier\n",
        "ironic_train_DF = training_A_DF[training_A_DF['Label'] == 1]\n",
        "non_ironic_train_DF = training_A_DF[training_A_DF['Label'] == 0]\n",
        "\n",
        "ironic_train = ironic_train_DF['Tweet text'].tolist()\n",
        "non_ironic_train = non_ironic_train_DF['Tweet text'].tolist()\n",
        "\n",
        "ironic_test_DF = test_A_DF[test_A_DF['Label'] == 1]\n",
        "non_ironic_test_DF = test_A_DF[test_A_DF['Label'] == 0]\n",
        "\n",
        "ironic_test = ironic_test_DF['Tweet text'].tolist()\n",
        "non_ironic_test = non_ironic_test_DF['Tweet text'].tolist()\n",
        "\n",
        "# Count the number of rows for each list\n",
        "num_of_ironic_doc = len(ironic_train)\n",
        "num_of_non_ironic_doc = len(non_ironic_train)\n",
        "total_num_doc = num_of_non_ironic_doc + num_of_ironic_doc\n",
        "\n",
        "prob_ironic = num_of_ironic_doc / total_num_doc\n",
        "prob_non_ironic = num_of_non_ironic_doc / total_num_doc\n",
        "\n",
        "vocab = Counter();\n",
        "vocab_ironic = Counter();\n",
        "vocab_non_ironic = Counter();\n",
        "\n",
        "total_ironic_word = 0;\n",
        "total_non_ironic_word = 0;\n",
        "\n",
        "# Split ironic training data into bag of words\n",
        "for sent in ironic_train:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_ironic[word] += 1\n",
        "        total_ironic_word += 1\n",
        "\n",
        "# Split non ironic training data into bag of words\n",
        "for sent in non_ironic_train:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_non_ironic[word] += 1\n",
        "        total_non_ironic_word += 1\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# TUNING REQUIRED\n",
        "alpha = 1.5\n",
        "\n",
        "# Find probability for ironic training data\n",
        "def find_ironic_prob(list_of_words):\n",
        "    ironic_prob_list_of_words = prob_ironic\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_ironic:\n",
        "            numerator = vocab_ironic[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_ironic_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        ironic_prob_list_of_words = ironic_prob_list_of_words * word_prob\n",
        "\n",
        "    return ironic_prob_list_of_words\n",
        "\n",
        "# Find probability for non ironic training data\n",
        "def find_non_ironic_prob(list_of_words):\n",
        "    non_ironic_prob_list_of_words = prob_non_ironic\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_non_ironic:\n",
        "            numerator = vocab_non_ironic[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_non_ironic_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        non_ironic_prob_list_of_words = non_ironic_prob_list_of_words * word_prob\n",
        "\n",
        "    return non_ironic_prob_list_of_words\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Ironic testing data\n",
        "for sent in ironic_test:\n",
        "    word_list = sent.split()\n",
        "    ironic_prob_sent = find_ironic_prob(word_list)\n",
        "    non_ironic_prob_sent = find_non_ironic_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"1\"\n",
        "\n",
        "    if ironic_prob_sent > non_ironic_prob_sent:\n",
        "      pred_sentiment = \"1\"\n",
        "    else:\n",
        "      pred_sentiment = \"0\"\n",
        "\n",
        "    predictions.append((sent, true_sentiment, pred_sentiment))\n",
        "\n",
        "# Non ironic testing data\n",
        "for sent in non_ironic_test:\n",
        "    word_list = sent.split()\n",
        "    ironic_prob_sent = find_ironic_prob(word_list)\n",
        "    non_ironic_prob_sent = find_non_ironic_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"0\"\n",
        "\n",
        "    if ironic_prob_sent > non_ironic_prob_sent:\n",
        "      pred_sentiment = \"1\"\n",
        "    else:\n",
        "      pred_sentiment = \"0\"\n",
        "\n",
        "    predictions.append((sent, true_sentiment, pred_sentiment))\n",
        "\n",
        "correct_pred = 0\n",
        "total_test_case = 0\n",
        "\n",
        "for pred in predictions:\n",
        "    (sent, true_sentiment, pred_sentiment) = pred\n",
        "    if true_sentiment == pred_sentiment:\n",
        "        correct_pred += 1\n",
        "    total_test_case += 1\n",
        "\n",
        "accuracy = correct_pred / total_test_case\n",
        "print(\"***** For Task A *****\")\n",
        "print(\"Accuracy of \", accuracy)\n",
        "print(\"Alpha is \", alpha)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** For Task A *****\n",
            "Accuracy of  0.6186224489795918\n",
            "Alpha is  1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWUSpPzbSS2C",
        "outputId": "2151af2c-7577-467b-f0a6-e3fb13c63e42"
      },
      "source": [
        "# Import training data for task B and store as DF\n",
        "training_B_URL = 'https://raw.githubusercontent.com/Cyvhee/SemEval2018-Task3/master/datasets/train/SemEval2018-T3-train-taskB_emoji.txt'\n",
        "\n",
        "training_B_DF = pd.read_table(training_B_URL)\n",
        "\n",
        "# Import testing data B and store as DF\n",
        "test_B_URL = 'https://raw.githubusercontent.com/Cyvhee/SemEval2018-Task3/master/datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt'\n",
        "\n",
        "test_B_DF = pd.read_table(test_B_URL)\n",
        "\n",
        "# Converts DF into list of strings containing Tweets separated by irony classifier\n",
        "ironic_train1_DF = training_B_DF[training_B_DF['Label'] == 1]\n",
        "ironic_train2_DF = training_B_DF[training_B_DF['Label'] == 2]\n",
        "ironic_train3_DF = training_B_DF[training_B_DF['Label'] == 3]\n",
        "non_ironic_train_DF = training_B_DF[training_B_DF['Label'] == 0]\n",
        "\n",
        "ironic_train1 = ironic_train1_DF['Tweet text'].tolist()\n",
        "ironic_train2 = ironic_train2_DF['Tweet text'].tolist()\n",
        "ironic_train3 = ironic_train3_DF['Tweet text'].tolist()\n",
        "non_ironic_train = non_ironic_train_DF['Tweet text'].tolist()\n",
        "\n",
        "ironic_test1_DF = test_B_DF[test_B_DF['Label'] == 1]\n",
        "ironic_test2_DF = test_B_DF[test_B_DF['Label'] == 2]\n",
        "ironic_test3_DF = test_B_DF[test_B_DF['Label'] == 3]\n",
        "non_ironic_test_DF = test_B_DF[test_B_DF['Label'] == 0]\n",
        "\n",
        "ironic_test1 = ironic_test1_DF['Tweet text'].tolist()\n",
        "ironic_test2 = ironic_test2_DF['Tweet text'].tolist()\n",
        "ironic_test3 = ironic_test3_DF['Tweet text'].tolist()\n",
        "non_ironic_test = non_ironic_test_DF['Tweet text'].tolist()\n",
        "\n",
        "# Count the number of rows for each list\n",
        "num_of_ironic1_doc = len(ironic_train1)\n",
        "num_of_ironic2_doc = len(ironic_train2)\n",
        "num_of_ironic3_doc = len(ironic_train3)\n",
        "num_of_non_ironic_doc = len(non_ironic_train)\n",
        "total_num_doc = num_of_non_ironic_doc + num_of_ironic1_doc + num_of_ironic2_doc + num_of_ironic3_doc\n",
        "\n",
        "prob_ironic1 = num_of_ironic1_doc / total_num_doc\n",
        "prob_ironic2 = num_of_ironic2_doc / total_num_doc\n",
        "prob_ironic3 = num_of_ironic3_doc / total_num_doc\n",
        "prob_non_ironic = num_of_non_ironic_doc / total_num_doc\n",
        "\n",
        "vocab = Counter();\n",
        "vocab_ironic1 = Counter();\n",
        "vocab_ironic2 = Counter();\n",
        "vocab_ironic3 = Counter();\n",
        "vocab_non_ironic = Counter();\n",
        "\n",
        "total_ironic1_word = 0;\n",
        "total_ironic2_word = 0;\n",
        "total_ironic3_word = 0;\n",
        "total_non_ironic_word = 0;\n",
        "\n",
        "# Split ironic 1 training data into bag of words\n",
        "for sent in ironic_train1:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_ironic1[word] += 1\n",
        "        total_ironic1_word += 1\n",
        "\n",
        "# Split ironic 2 training data into bag of words\n",
        "for sent in ironic_train2:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_ironic2[word] += 1\n",
        "        total_ironic2_word += 1\n",
        "\n",
        "# Split ironic 3 training data into bag of words\n",
        "for sent in ironic_train1:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_ironic3[word] += 1\n",
        "        total_ironic3_word += 1\n",
        "\n",
        "# Split non ironic training data into bag of words\n",
        "for sent in non_ironic_train:\n",
        "    word_list = sent.split()\n",
        "\n",
        "    for word in word_list:\n",
        "        vocab[word] += 1\n",
        "        vocab_non_ironic[word] += 1\n",
        "        total_non_ironic_word += 1\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# TUNING REQUIRED\n",
        "alpha = 0.1\n",
        "\n",
        "# Find probability for ironic 1 training data\n",
        "def find_ironic1_prob(list_of_words):\n",
        "    ironic1_prob_list_of_words = prob_ironic1\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_ironic1:\n",
        "            numerator = vocab_ironic1[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_ironic1_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        ironic1_prob_list_of_words = ironic1_prob_list_of_words * word_prob\n",
        "\n",
        "    return ironic1_prob_list_of_words\n",
        "\n",
        "# Find probability for ironic 2 training data\n",
        "def find_ironic2_prob(list_of_words):\n",
        "    ironic2_prob_list_of_words = prob_ironic2\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_ironic2:\n",
        "            numerator = vocab_ironic2[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_ironic2_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        ironic2_prob_list_of_words = ironic2_prob_list_of_words * word_prob\n",
        "\n",
        "    return ironic2_prob_list_of_words\n",
        "\n",
        "# Find probability for ironic 3 training data\n",
        "def find_ironic3_prob(list_of_words):\n",
        "    ironic3_prob_list_of_words = prob_ironic1\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_ironic3:\n",
        "            numerator = vocab_ironic3[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_ironic3_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        ironic3_prob_list_of_words = ironic3_prob_list_of_words * word_prob\n",
        "\n",
        "    return ironic3_prob_list_of_words\n",
        "\n",
        "# Find probability for non ironic training data\n",
        "def find_non_ironic_prob(list_of_words):\n",
        "    non_ironic_prob_list_of_words = prob_non_ironic\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in vocab_non_ironic:\n",
        "            numerator = vocab_non_ironic[word] + alpha\n",
        "        else:\n",
        "            numerator = alpha\n",
        "\n",
        "        denominator = total_non_ironic_word + alpha * vocab_size\n",
        "        word_prob = numerator / denominator\n",
        "\n",
        "        non_ironic_prob_list_of_words = non_ironic_prob_list_of_words * word_prob\n",
        "\n",
        "    return non_ironic_prob_list_of_words\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Ironic 1 testing data\n",
        "for sent in ironic_test1:\n",
        "    word_list = sent.split()\n",
        "    ironic1_prob_sent = find_ironic1_prob(word_list)\n",
        "    ironic2_prob_sent = find_ironic2_prob(word_list)\n",
        "    ironic3_prob_sent = find_ironic3_prob(word_list)\n",
        "    non_ironic_prob_sent = find_non_ironic_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"1\"\n",
        "\n",
        "    max_prob = max(ironic1_prob_sent, ironic2_prob_sent, ironic3_prob_sent, non_ironic_prob_sent)\n",
        "\n",
        "    if max_prob == ironic1_prob_sent:\n",
        "      pred_sentiment = \"1\"\n",
        "    if max_prob == ironic2_prob_sent:\n",
        "      pred_sentiment = \"2\"\n",
        "    if max_prob == ironic3_prob_sent:\n",
        "      pred_sentiment = \"3\"\n",
        "    if max_prob == non_ironic_prob_sent:\n",
        "      pred_sentiment = \"0\"\n",
        "\n",
        "# Ironic 2 testing data\n",
        "for sent in ironic_test2:\n",
        "    word_list = sent.split()\n",
        "    ironic1_prob_sent = find_ironic1_prob(word_list)\n",
        "    ironic2_prob_sent = find_ironic2_prob(word_list)\n",
        "    ironic3_prob_sent = find_ironic3_prob(word_list)\n",
        "    non_ironic_prob_sent = find_non_ironic_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"2\"\n",
        "\n",
        "    max_prob = max(ironic1_prob_sent, ironic2_prob_sent, ironic3_prob_sent, non_ironic_prob_sent)\n",
        "\n",
        "    if max_prob == ironic1_prob_sent:\n",
        "      pred_sentiment = \"1\"\n",
        "    if max_prob == ironic2_prob_sent:\n",
        "      pred_sentiment = \"2\"\n",
        "    if max_prob == ironic3_prob_sent:\n",
        "      pred_sentiment = \"3\"\n",
        "    if max_prob == non_ironic_prob_sent:\n",
        "      pred_sentiment = \"0\"\n",
        "\n",
        "# Ironic 3 testing data\n",
        "for sent in ironic_test3:\n",
        "    word_list = sent.split()\n",
        "    ironic1_prob_sent = find_ironic1_prob(word_list)\n",
        "    ironic2_prob_sent = find_ironic2_prob(word_list)\n",
        "    ironic3_prob_sent = find_ironic3_prob(word_list)\n",
        "    non_ironic_prob_sent = find_non_ironic_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"3\"\n",
        "\n",
        "    max_prob = max(ironic1_prob_sent, ironic2_prob_sent, ironic3_prob_sent, non_ironic_prob_sent)\n",
        "\n",
        "    if max_prob == ironic1_prob_sent:\n",
        "      pred_sentiment = \"1\"\n",
        "    if max_prob == ironic2_prob_sent:\n",
        "      pred_sentiment = \"2\"\n",
        "    if max_prob == ironic3_prob_sent:\n",
        "      pred_sentiment = \"3\"\n",
        "    if max_prob == non_ironic_prob_sent:\n",
        "      pred_sentiment = \"0\"\n",
        "\n",
        "# Non ironic testing data\n",
        "for sent in non_ironic_test:\n",
        "    word_list = sent.split()\n",
        "    ironic_prob_sent = find_ironic_prob(word_list)\n",
        "    non_ironic_prob_sent = find_non_ironic_prob(word_list)\n",
        "\n",
        "    true_sentiment = \"0\"\n",
        "\n",
        "    if ironic_prob_sent > non_ironic_prob_sent:\n",
        "      pred_sentiment = \"1\"\n",
        "    else:\n",
        "      pred_sentiment = \"0\"\n",
        "\n",
        "    predictions.append((sent, true_sentiment, pred_sentiment))\n",
        "\n",
        "correct_pred = 0\n",
        "total_test_case = 0\n",
        "\n",
        "for pred in predictions:\n",
        "    (sent, true_sentiment, pred_sentiment) = pred\n",
        "    if true_sentiment == pred_sentiment:\n",
        "        correct_pred += 1\n",
        "    total_test_case += 1\n",
        "\n",
        "accuracy = correct_pred / total_test_case\n",
        "print(\"***** For Task B *****\")\n",
        "print(\"Accuracy of \", accuracy)\n",
        "print(\"Alpha is \", alpha)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** For Task B *****\n",
            "Accuracy of  0.4989429175475687\n",
            "Alpha is  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}